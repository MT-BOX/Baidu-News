# Baidu-News
基于爬虫的百度新闻搜索引擎：
爬取网页，起始网页为百度中 百家号上随意一篇文章 http://baijiahao.baidu.com/s?id=1685579912006379153
设置初始配置 
请求头：防止反爬
已爬取标题集合、已爬取url集合：防止爬取重复内容
待爬取集合（每到达一个网页 爬取该网页下的url,保存为待爬取集合）
记录 爬取网页的 标题、链接、正文内容（用于文本摘要及检索）、下级urls
（1）使用tfidf，根据文本相似度进行文本去重，将重复度大于0.9的文本删除 （这里的操作是删除所有，实际上应该保留一篇）
（2）数据处理
在dataframe中进行操作，根据每个链接保存的子链接，进行两步操作，
一步为筛选子链接，只保留在当前已爬取保存文件中有的子链接
二步为根据一条url，反向搜索其包含在哪些url的子链接中
从而构建起 各节点之间的连通图
（3）进行摘要提取 
尝试使用textrank4zh库中的TextRank4Sentence进行摘要提取，以及知乎开源的TextSummary类其实大部分是进行textrank的操作
（4）建立倒排索引
其实就是对所有文章进行分词，对应每一个单词，其在哪些文章中出现过做一个映射
根据搜索的关键词，可以快速在倒排索引中取出其对应的文章索引
（5）搜索部分
关键词分词
取出倒排索引
定位文章
文章根据打分排序
人为设定排序规则 
摘要-40% 正文-25% 标题-35%+匹配的词数
先按照分词后 与多少个词匹配排序 之后再按照摘要、正文、标题内包含的频率 进行加权统计
